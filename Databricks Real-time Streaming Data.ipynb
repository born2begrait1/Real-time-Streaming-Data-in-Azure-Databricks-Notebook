{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d8f8386-ad60-4a88-85be-9b190c15b9e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Real-time Streaming Data leveraging Medallion Architeture\n",
    "\n",
    "\n",
    "![Image](https://raw.githubusercontent.com/born2begrait1/Real-time-Streaming-Data-in-Azure-Databricks-Notebook/5c1242c17cce48880e7de78f9e17ae3f85458cf6/realtime%20in%20databricks.png)\n",
    "\n",
    "\n",
    "### Setup and Configuration:\n",
    "- Azure Subscription: Ensure you have an active Azure subscription.\n",
    "- Azure Event Hub: Create an Event Hub namespace and an event hub for capturing real-time data.\n",
    "- Azure Databricks: Set up an Azure Databricks workspace with Unity Catalog enabled.\n",
    "- Azure Data Lake Storage: Configure Azure Data Lake Storage for storing processed data.\n",
    "\n",
    "### Azure Databricks Cluster Required Configuration\n",
    "- Single Node Compute Cluster: `12.2 LTS (includes Apache Spark 3.3.2, Scala 2.12)`\n",
    "- Maven Library installed on Compute Cluster: `com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.22`\n",
    "\n",
    "### Development Environment:\n",
    "- VS Code: Install Visual Studio Code and set up the necessary extensions for Python and Azure.\n",
    "- Maven Libraries: Add the following Maven libraries to your project:\n",
    "a. com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.22\n",
    "b.com.azure:azure-messaging-eventhubs:5.18.0\n",
    "c.com.azure:azure-identity:1.11.2\n",
    "\n",
    "### Implementation Steps:\n",
    "- Create Event Hub Connection:\n",
    "a. Obtain the connection string for your Event Hub namespace.\n",
    "b. Configure the connection string and Event Hub name in your Databricks notebook.\n",
    "- Ingest Data from Event Hub:\n",
    "a. Use Spark Structured Streaming to read data from Azure Event Hub.\n",
    "b. Define the schema for the incoming data.\n",
    "\n",
    "### Process Data in Databricks:\n",
    "- Perform necessary transformations and aggregations on the streaming data.\n",
    "- Use the Medallion Architecture (Bronze, Silver, Gold layers) to organize and process data.\n",
    "\n",
    "### Store Processed Data:\n",
    "- Write the processed data to Azure Data Lake Storage in Delta format.\n",
    "- Use Delta Lake for efficient storage and querying.\n",
    "\n",
    "### Visualize Data:\n",
    "-Visualize Data in Power BI using DirectQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d36e02c-ac69-4b9d-9180-5c7349b56b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Importing required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aabda48-e6e4-4960-9788-93d5400fd296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import avg, count, round, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c9a6fda-b15b-4efd-9ae5-8236cd464b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Azure Event Hubs Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "059adb71-2a6e-408f-b708-72b6cadce286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure Event Hubs Connection String, Event Hub Namespace, name, and key\n",
    "CONNECTION_STR = \"<Event Hub Primary Connection String>\"\n",
    "EVENTHUB_NAME = \"EventHub Name\"\n",
    "\n",
    "connectionConf = {\n",
    "  'eventhubs.connectionString' : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(CONNECTION_STR),\n",
    "  'eventhubs.name': EVENTHUB_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e365dce9-433a-4a82-bc41-e53e3f413099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create Medallion Architecture Catalog, Schemas and Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "039b051b-f481-4344-8a2f-3da87599ed57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Storage location for catalog\n",
    "storage_location = '<storage-location-abfss-path>'\n",
    "\n",
    "# Create Unity Catalog\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS weather MANAGED LOCATION '{storage_location}'\")\n",
    "\n",
    "# Create bronze layer schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS weather.bronze\")\n",
    "\n",
    "# Create silver layer schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS weather.silver\")\n",
    "\n",
    "# Create gold layer schema\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS weather.gold\")\n",
    "\n",
    "# Create bronze layer volume for checkpoint\n",
    "spark.sql(\"CREATE VOLUME weather.bronze.bronze_volume\")\n",
    "\n",
    "# Create silver layer volume for checkpoint\n",
    "spark.sql(\"CREATE VOLUME weather.silver.silver_volume\")\n",
    "\n",
    "# Create gold layer volume for checkpoint\n",
    "spark.sql(\"CREATE VOLUME weather.gold.gold_volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e72830e-c300-470a-9b44-071324c29e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b6fccac-9939-43ce-9cd4-61042fea4478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading stream to load data from Azure Event Hub into df Spark DataFrame\n",
    "df = spark.readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**connectionConf) \\\n",
    "    .load()\n",
    "\n",
    "# Displaying stream \n",
    "df.display()\n",
    "\n",
    "\n",
    "# Writing the streaming data to the Delta table in append mode \n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"<volumme-path>\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"weather.bronze.bronze_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08410bd0-8f21-4119-81e3-6650b57abd9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f104ed4e-2338-4d15-a3cc-ea1c0824cf3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the JSON schema\n",
    "json_schema = StructType([\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"humidity\", DoubleType(), True),\n",
    "    StructField(\"wind_speed\", DoubleType(), True),\n",
    "    StructField(\"pressure\", DoubleType(), True),\n",
    "    StructField(\"precipitation\", DoubleType(), True),\n",
    "    StructField(\"cloud_cover\", DoubleType(), True),\n",
    "    StructField(\"weather_condition\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5747608a-5a3a-431d-8635-5861fc91d5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading streaming data from the Delta table\n",
    "df = spark.readStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .table(\"weather.bronze.bronze_data\")\\\n",
    "    .withColumn(\"body\", col(\"body\").cast(\"string\"))\\\n",
    "    .withColumn(\"body\", from_json(col(\"body\"), json_schema))\n",
    "\n",
    "# Casting all columns to string\n",
    "df_string = df.select(\n",
    "    col(\"body.city\").cast(\"string\").alias(\"city\"),\n",
    "    col(\"body.latitude\").cast(\"string\").alias(\"latitude\"),\n",
    "    col(\"body.longitude\").cast(\"string\").alias(\"longitude\"),\n",
    "    col(\"body.temperature\").cast(\"string\").alias(\"temperature\"),\n",
    "    col(\"body.humidity\").cast(\"string\").alias(\"humidity\"),\n",
    "    col(\"body.wind_speed\").cast(\"string\").alias(\"wind_speed\"),\n",
    "    col(\"body.pressure\").cast(\"string\").alias(\"pressure\"),\n",
    "    col(\"body.precipitation\").cast(\"string\").alias(\"precipitation\"),\n",
    "    col(\"body.cloud_cover\").cast(\"string\").alias(\"cloud_cover\"),\n",
    "    col(\"body.weather_condition\").cast(\"string\").alias(\"weather_condition\"),\n",
    "    col(\"body.timestamp\").cast(\"string\").alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "display(df_string)\n",
    "\n",
    "\n",
    "# Writing the streaming data to the Delta table in append mode\n",
    "df_string.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"volume-path\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"weather.silver.silver_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65ea0bcd-ad2e-4c15-bbad-56eb0fd04321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed330fde-0364-4b59-be1c-5f2e9b057aab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading streaming data from the Silver table\n",
    "df_silver = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .table(\"weather.silver.silver_data\")\n",
    "\n",
    "# Performing advanced aggregation and rounding to 2 decimal places\n",
    "df_aggregated = df_silver.groupBy(col(\"city\")).agg(\n",
    "    round(avg(col(\"temperature\")), 2).alias(\"avg_temperature\"),\n",
    "    round(avg(col(\"humidity\")), 2).alias(\"avg_humidity\"),\n",
    "    round(avg(col(\"wind_speed\")), 2).alias(\"avg_wind_speed\"),\n",
    "    count(\"*\").alias(\"record_count\")\n",
    ")\n",
    "\n",
    "display(df_aggregated)\n",
    "\n",
    "# Writing the aggregated streaming data to the Delta table in append mode\n",
    "df_aggregated.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"volume-path\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .toTable(\"weather.gold.gold_data\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "Databricks Real-time Streaming Data",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}